---
title: "Predicting the casualties during prolific days using SPC Outlooks and environmental factors"
author: "Zoe Schroder/James Elsner"
date: "11/18/2018"
output: github_notebook
editor_options: 
  chunk_output_type: inline
chunk_output_type: console
---

Set working directory and load packages. Suppress the messages so they do not clutter the screen! 
```{r}
suppressMessages(library(lubridate))
suppressMessages(library(sf))
suppressMessages(library(tmap))
suppressMessages(library(USAboundaries))
suppressMessages(library(rgeos))
suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
suppressMessages(library(raster))
suppressMessages(library(lubridate))
suppressMessages(library(xts))
```

##########################
## Part 1: Tornado Data ##
##########################

Download the tornado data from the Storm Prediction Center (SPC) http://www.spc.noaa.gov/gis/svrgis/. This database includes all tornadoes from 1950 to 2016 in a shapefile. A description of all column headings and units can be found here: http://www.spc.noaa.gov/wcm/data/SPC_severe_database_description.pdf 
```{r}
download.file(url = "http://www.spc.noaa.gov/gis/svrgis/zipped/1950-2017-torn-initpoint.zip",
              destfile = "tornado.zip", mode = "wb")
unzip("tornado.zip")
```

Load the shapefile into R using the `read_sf` function from the **sf** package. Set `stringsAsFactors` to **FALSE** so that the character vectors are not converted to factors. 
```{r}
Torn.sf <- read_sf(dsn = "1950-2017-torn-initpoint", 
                   layer = "1950-2017-torn-initpoint", 
                   stringsAsFactors = FALSE)
```
Torn.sf is a 62519 x 22 data frame.

Remove tornadoes in Hawaii, Alaska, and Puerto Rico and those occurring before 1994. That year marks the beginning of the comprehensive WSR-88D radar. For missing EF ratings use the modification rules (if/else) defined here: https://www.spc.noaa.gov/wcm/OneTor_F-scale-modifications.pdf. **NOTE: All tornadoes with an EF rating of -9 have a property loss of 0. (These are all 2016 and 2017 tornadoes.) Therefore, you do not need that column to assign a magnitude. **
```{r}
Torn.sf <- Torn.sf %>%
  filter(yr >= 1994,
         !st %in% c("AK", "PR", "HI")) %>%
  mutate(mag = ifelse(mag == -9 & len <= 5, 0, mag),
         mag = ifelse(mag == -9 & len > 5, 1, mag))
```

Add a date/time column also add columns for path length, width, and area in metric units. Leave the time zone as native CDT. Create a convective day (6AM to 6AM) column taking hours 00:00:00 -> 05:59:59 and assigning it to the previous date (this associates the previous day's date to tornadoes occurring up to 6 hours after local midnight). The length needs to be converted to meters from miles and 0 lengths need to be assigned a value. The width of the tornado tracks need to be changed to meters from yard and 0 widths need to be assigned a value. Add a casualties column (injuries and fatalities). Calculate the area of the tornado path. Create a column with the name of the months. 
```{r}
Torn.sf <- Torn.sf %>%
  mutate(dy = format(as.Date(date,format="%Y-%m-%d"), "%d"),
         DateTime = as.POSIXct(paste(yr, mo, dy, time), format = "%Y%m%d%H:%M:%S"),
         Hour = hour(DateTime),
         Year = year(DateTime),
         cDateTime = DateTime - as.difftime(6, unit = "hours"),
         cDate = as.Date(as_datetime(ifelse(Hour < 6, (DateTime - 86400), cDateTime), tz = Sys.timezone())),
         Length = len * 1609.34,
         Length = ifelse(Length == 0, min(Length[Length > 0]), Length), #takes care of zero length
         Width = wid * .9144,
         Width = ifelse(Width == 0, min(Width[Width > 0]), Width), #takes care of zero width
         Width = ifelse(Year >= 1995, Width * pi/4, Width), #takes care of change: avg to max
         cas = inj + fat,
         AreaPath = Length * Width,
         Ma = factor(month.abb[mo], levels = month.abb[1:12])) %>%
  sf::st_sf()
max(Torn.sf$yr)
```

The geometry type is `POINT`. Each tornado is represented as a single point location geometry (start location). 

Add energy dissipation per tornado. Use the empirical model for tornado winds by EF rating taken from Table 3-1 of NRC 2007. Percent area by EF rating for each EF category. Threshold wind speeds (m/s) are a lower bound 3-sec gusts on the operational EF Scale (Table 2-1 of NRC2007). This is based on work by Fricker et al. (2017). The model is
$$
E = A_p \rho \sum_{j=0}^{J} w_j v_j^{3},
$$
where $A_p$ is the area of the path, $\rho$ is area density [1 kg/m^3]  $v_j$ is the midpoint wind speed for each rating, and $w_j$ is the corresponding fraction of path area by EF rating. With no upper bound on the EF5 wind speeds, the midpoint wind speed is set at 97 m~s$^{-1}$ (7.5 m~s$^{-1}$ above the threshold wind speed consistent with the EF4 midpoint speed relative to its threshold)

Add the energy dissipation per tornado. 
```{r}
perc <- c(1, 0, 0, 0, 0, 0, 
          .772, .228, 0, 0, 0, 0,
          .616, .268, .115, 0, 0, 0,
          .529, .271, .133, .067, 0, 0,
          .543, .238, .131, .056, .032, 0,
          .538, .223, .119, .07, .033, .017)
percM <- matrix(perc, ncol = 6, byrow = TRUE)
threshW <- c(29.06, 38.45, 49.62, 60.8, 74.21, 89.41)
midptW <- c(diff(threshW)/2 + threshW[-length(threshW)], threshW[length(threshW)] + 7.5)
ef <- Torn.sf$mag + 1
EW3 <- numeric()
for(i in 1:length(ef)) EW3[i] = midptW^3 %*% percM[ef[i], ]
Torn.sf <- Torn.sf %>%
  mutate(ED = EW3 * AreaPath)
```

Determine the distance between tornadoes in space and time. Use a projection, not lat/lon. See https://epsg.io/102004. Extract the coordinates of the start locations as a N by 2 matrix, where N is the number of tornadoes. Also extract the date-time as a vector of class `POSIXct`.
```{r}
Torn.sfT <- st_transform(Torn.sf, crs = 102004)
space <- st_coordinates(Torn.sfT)
time <- Torn.sf$DateTime
```

Next compute pairwise Euclidean distances in space and, separately, in time using the `dist()` function. Divide the spatial distance by 10 so that the values are commensurate with the time 'distance' based on the assumption of 10 meters every second for an average speed of tornado-generating storms. 

Compare: Distance from New York to Denver is 2.622 x 10^6 meters. There are 3.154 x 10^7 seconds in a year. This will capture the historic multiday tornado outbreaks. For analysis we want to consider each day in the multiday group separately. As the value of the divisor increases cluster areas get larger. Remove `ds` and `dt` to free memory.
```{r}
ds <- dist(space) / 10
dt <- dist(time)
dst <- ds + dt
rm(ds, dt)
```

Distances are saved as an object of class `dist` containing a vector of length N * (N-1)/2, which is the number of unique point pairs.

Next group the tornadoes based on the space-time distances. This is done with the `hclust()` (hierarchical cluster) function. Initially, each tornado is assigned to its own group and then the algorithm joins the two closest tornadoes determined by values in `dst`. The algorithm continues by joining tornadoes (and tornado groups) until there is a single large group.

The single linkage method (`method = "single"`) is related to the minimal spanning tree (MST) and adopts a 'friends of friends' grouping strategy. An edge-weighted graph is a graph where each edge has a weight (or cost). Here weights are space-time distances between tornadoes. A MST of an edge-weighted graph is a spanning tree whose weight (the sum of the weights of its edges) is no larger than the weight of any other spanning tree. A spanning tree of a graph on N vertices (tornado centroids) is a subset of N-1 edges that form a tree (Skiena 1990, p. 227).
 
The `cutree()` function is used to extract a group number for each tornado. Tornadoes in each group are close in space & time. Here the tree is cut at a height of 50000 space-time units. Making `h` smaller results in smaller groups (fewer tornadoes per group).
```{r}
stime <- proc.time()
tree <- hclust(dst, method = "single")
groupNumber <- as.integer(cutree(tree, h = 50000))
proc.time() - stime
```

Add the group number to each tornado. 
```{r}
Torn.sfT$groupNumber <- groupNumber
```

Create an ID for unique big days. It needs to have the group number and the cDate because there are multiple group numbers in each cDate. Similarly, there are multiple cDates in each groupnumber. An ID helps retrieve appropriate/unique big days. 
```{r}
Torn.sfT <- Torn.sfT %>%
   mutate(ID = paste0(gsub("-", "", cDate), groupNumber))
```

Compute big day-level statistics. Keep only days with at least 10 tornadoes. 
```{r}
BigDays.sfdfT <- Torn.sfT %>%
  group_by(ID, groupNumber, cDate) %>%
  summarize(Year = first(Year),
            Month = first(mo),
            FirstDate = first(date),
            LastDate = last(date),
            DateRange = paste(FirstDate, "to", LastDate),
            FirstcDate = first(cDate),
            LastcDate = last(cDate),
            ncD = n_distinct(cDate),
            casualties = sum(cas),
            nT = n(),
            nT1_5 = sum(mag >= 1),
            n0 = sum(mag == 0),
            n1 = sum(mag == 1),
            n2 = sum(mag == 2),
            n3 = sum(mag == 3),
            n4 = sum(mag == 4),
            n5 = sum(mag == 5),
            GroupTotalED = sum(ED),
            GroupTotalEDinTW = paste(round(GroupTotalED/10^12), "TW"),
            maxEF = max(mag),
            nD = n_distinct(date),
            startTime = first(DateTime),
            middleTime = median(DateTime),
            finishTime = last(DateTime),
            Duration = difftime(finishTime, startTime, units = "secs")) %>%
  filter(nT >= 10)
dim(BigDays.sfdfT)
```

Convert `middleTime` to UTC.
```{r}
attr(BigDays.sfdfT$startTime, "tzone") <- "UTC"
attr(BigDays.sfdfT$middleTime, "tzone") <- "UTC"
attr(BigDays.sfdfT$finishTime, "tzone") <- "UTC"
```

Round the UTC time to nearest 6 hours. This is done with the `align.time()` function from the **xts** package. Adjust it by 3 hours to get the closest time. This falls within the outbreak so you need to subtract by 3 hours (10800 seconds). This will produce the closest 3 hour NARR time that occurs before and not within the big day. 
```{r}
BigDays.sfdfT$NARRtime <- (align.time(BigDays.sfdfT$startTime, n = (60 * 60 * 3)) - 3600 * 3)
```


Split the NARR date and time into their individual variables. Then bind the columns for BigDays.sfdfT. NOTE: cannot do a mutate because 00Z produces NAs. DON'T USE!
```{r, eval = FALSE}
NARRday = format(as.POSIXct(strptime(BigDays.sfdfT$NARRtime,"%Y-%m-%d %H:%M:%S",tz="")) ,format = "%Y/%m/%d")
NARRZtime = format(as.POSIXct(strptime(BigDays.sfdfT$NARRtime,"%Y-%m-%d %H:%M:%S",tz="")) ,format = "%H")

BigDays.sfdfT <- cbind(BigDays.sfdfT, NARRday, NARRZtime )
```

Create a table to show how many big days fall in each start Z time. 
```{r}
BigDays.sfdfT %>%
  group_by(NARRZtime) %>%
  summarize(count = n())
```

Create a downloadable string of information for the varying NARR times. DON'T USE
```{r}
BigDays.sfdfT <- BigDays.sfdfT %>%
  mutate(YrMoDa = gsub("/", "", NARRday),
         ztime = NARRZtime,
         slug = paste0("merged_AWIP32.",YrMoDa, ztime),
         slug2 = paste0("merged_AWIP32.",YrMoDa))
```

Extract a vector of the big days. Save as a .csv for NARR download. 
```{r}
bigdays <- BigDays.sfdfT$NARRday
bigdaytimes <- BigDays.sfdfT$NARRZtime
x <- cbind(as.character(bigdays), as.character(bigdaytimes))
write.csv(x, "BigDays.csv")
```

```{r}
BigDays.sfdfT <- st_transform(BigDays.sfdfT, 
  crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
```

Obtain the group day hulls. Transform the CRS to match that of the environmental data raster grids.
```{r}
BigDays.sfdfT <- st_convex_hull(BigDays.sfdfT)
BigDays.sfdfT$HullArea <- st_area(BigDays.sfdfT)
BigDays.sfdfT <- st_transform(BigDays.sfdfT, 
  crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
```

Get the centroid (central point of the tornado activity) for each big day. 
```{r}
BigDayCentroids.sfdfT <- st_centroid(BigDays.sfdfT)
BigDayCentroids.sfdfT$groupArea <- st_area(st_convex_hull(BigDays.sfdfT))
BigDayCentroids.sfdfT$groupDensity <- BigDayCentroids.sfdfT$nT/BigDayCentroids.sfdfT$groupArea
```

Save the Big Days to a file `BigDays.RData`.
```{r}
#save(BigDays.sfdfT, file = "BigDays.RData")
load("BigDays.RData")
dim(BigDays.sfdfT)
```

Pull out the individual tornadoes associated with each Big Day. 
```{r, eval = FALSE}
groups <- BigDays.sfdfT$ID
Torn.sfT <- Torn.sfT %>%
  filter(ID %in% groups) 
```

Save the Tornadoes to a file `BigDayTornadoes.RData`.
```{r}
#save(Torn.sfT, file = "BigDayTornadoes.RData")
load("BigDayTornadoes.RData")
dim(Torn.sfT)
```

#######################
## Make some images: ##
#######################

Set the state and county borders. Remove Alaska, Hawaii, and Puerto Rico. 
```{r}
sts <- state.name[!state.name %in% c("Alaska", "Hawaii")]
stateBorders <- us_states(states = sts)

#stateBorders <- st_transform(stateBorders, crs = st_crs(BigDays.sfdfT))

counties <- us_counties()
counties.sf <- counties %>%
  filter(!state_abbr %in% c("AK", "PR", "HI")) 
#counties.sf <- st_transform(counties.sf, crs = st_crs(BigDays.sfdfT))
```

Plot the hulls for each big day on a map. 
```{r}
tm_shape(BigDays.sfdfT) +
  tm_polygons(alpha = .1) + 
tm_shape(stateBorders, projection = "laea_NA", is.master = TRUE) + 
  tm_borders()
```

Plot the centroids of all big days. Size by the number of casualties.
```{r}
counties.sf <- st_transform(counties.sf, 
                            crs = st_crs(stateBorders))
BigDays.sfdfT <- st_transform(BigDays.sfdfT, 
                            crs = st_crs(stateBorders))
x <- st_convex_hull(BigDays.sfdfT)

tm_shape(x) +
  tm_symbols(alpha = 0, border.alpha = 0) +
tm_shape(stateBorders) + 
  tm_borders(col = "gray15", alpha = 1) +
  tm_compass(size = 3) + tm_scale_bar(width = 0.45, size = 1) +
tm_shape(counties.sf) +
  tm_borders(col = "gray40", alpha = .3) +
  tm_format_World(legend.position = c("right", "bottom"),
                   attr.position = c("left", "bottom"),
                   legend.frame = FALSE,
                   #title = "Big Day centroids",
                   #title.size = 1.3,
                   #title.position = c("left", "TOP"),
                   inner.margins = c(.075, .0, .0, .0)) + 
tm_shape(BigDays.sfdfT) +
  tm_symbols(size = "casualties", col = "blue",
             title.size = "Count", 
             legend.size.is.portrait = FALSE, 
             shape =24, 
             scale = 3) +
  tm_layout(legend.title.size=1.5,
  legend.text.size = 1,
  legend.position = c("right","bottom")) 
```

Get the casualty count by state. 
```{r, eval = FALSE}
casualty <- Torn.sfT %>%
  group_by(st) %>%
  summarize(cas = n())

casualty[30:40, ]
```

Manipulate the data to add casualty count by state to the `us_states()` data frame from the **USAboundaries** package. 
```{r}
missstates <- c("AZ", "CA", "ID", "NV", "OR", "RI", "UT", "WA", "DC")
misscas <- as.integer(rep(0, 13))

missing <- cbind(missstates, misscas)

st <- as.character(casualty$st)
test <- c(st, missstates)

cascount <- casualty$cas
test2 <- c(cascount, misscas)

test3 <- cbind(test, test2)
test3 <- test3[order(test),]

states.sf <- us_states()

states.sf <- states.sf %>%
  filter(!stusps %in% c("AK", "PR", "HI")) %>%
  arrange(-desc(state_name))

states.sf <- states.sf[order(states.sf$state_abbr),]
x <- as.data.frame(test3)

states.sf <- cbind(states.sf, x$test2)
states.sf$cascount <- states.sf$x.test2

states.sf$col2 <- as.numeric(as.character(states.sf$cascount))
states.sf <- states.sf[order(states.sf$col2),]
```

Map the casualty count by state: 
```{r, eval = FALSE}
tm_shape(states.sf) + 
  tm_borders(col = "black", alpha = 1) +
     tm_fill("col2",
            title = "Count",
            breaks = c(0, 100, 200, 300, 400, 500, 600, 800),
            style = "fixed", 
            palette = "Reds", n = 37) +
  tm_text("cascount") +
  tm_compass() + tm_scale_bar() +
  tm_layout( legend.text.size = .75)
```

Time Series of casualty counts.
```{r}
Torn.sfT %>%
  group_by(Year) %>%
  summarize(cas = n()) %>%
ggplot(., aes(x = Year, y = cas)) + 
  geom_line() +
  geom_point() +
  labs(x = "Year", 
       y = "Total Casualties") +
  theme_minimal() +
  geom_smooth(method = lm) # +
# ggtitle("Casualty count by year") + 
# theme(plot.title = element_text(hjust = 0.5)) + 
# ylim(0,1300)
```

Plot the relationship between GroupDayTotalED (ATE) by Year. Size the dots by casualties. 
```{r}
BigDays.sfdfT %>%
  group_by(Year) %>%
  summarize(casualties = sum(casualties),
            avgATE = mean(GroupTotalED/10**12)) %>%
ggplot(., aes(x = Year, y = avgATE)) +
    geom_point(aes(size = casualties), color = "blue", alpha = .5) +
    scale_size_continuous(name = "Casualties") +
    scale_x_continuous(breaks = seq(1995, 2015, 5)) +
    xlab("Year") + ylab("Average ATE [in TW]") +
    theme_minimal() 
```

################
## Data Check ##
################

## NARR data: 
Data is downloaded from NCAR's North American Regional Reanalysis (https://rda.ucar.edu/datasets/ds608.0/#!access). It extends from 1-1-1979 to 11-1-2018. Use the NCAR NARR 3-hourly files.  

Spatial Extent: 
Longitude Range: Westernmost = 148.64E Easternmost = 2.568W
Latitude Range: Southernmost = 0.897N Northernmost = 85.333N 

```{r}
BigDays.sfdfT <- st_transform(BigDays.sfdfT, 
  crs = "+proj=lcc +lat_1=50 +lat_2=50 +lat_0=50 +lon_0=-107 +x_0=0 +y_0=0 +a=6371200 +b=6371200 +units=m +no_defs")
```

The list of all variables can be found here: http://www.emc.ncep.noaa.gov/mmb/rreanl/merged_land_AWIP32.pBigDays.sfdfT 

```{r}
slug <- BigDays.sfdfT$slug
slug2 <- BigDays.sfdfT$slug2
```


Read the grib files as raster bricks and assign the CAPE and helicity variables to separate raster layers. Extract the average (and extreme) environmental values within each of the big days in large groups hulls. 
```{r}
aCAPE <- numeric()
aHLCY <- numeric()
aCIN <- numeric()
aUSTM <- numeric()
aVSTM <- numeric()
aBS <- numeric()
aRATIO <- numeric()
mCAPE <- numeric()
mHLCY <- numeric()
mCIN <- numeric()
mUSTM <- numeric()
mVSTM <- numeric()
mBS <- numeric()
 
library(raster)
for(i in 1:length(slug)){
  print(i)
  rb <- brick(paste0("/Volumes/My Passport for Mac/NCARNARR/All/", BigDays.sfdfT$slug2[i], "/",BigDays.sfdfT$slug[i])) #<-- this is for varying NARR times
   #rb <- brick(paste0("/Volumes/My Passport for Mac/PhD_Program/NCARNARR/Files/18Z/", BigDays.sfdfT$slug[i]))
  CAPE.rl <- raster(rb, layer = 375)
  HLCY.rl <- raster(rb, layer = 323)
  CIN.rl <- raster(rb, layer = 376)
  USTM.rl <- raster(rb, layer = 324)
  VSTM.rl <- raster(rb, layer = 325)
  BS.rl <- sqrt(USTM.rl^2 + VSTM.rl^2)
  RATIO.rl <- CAPE.rl/abs(CIN.rl)
  aCAPE <- c(aCAPE, as.numeric(raster::extract(CAPE.rl, BigDays.sfdfT[i, ], fun = mean)))
  aHLCY <- c(aHLCY, as.numeric(raster::extract(HLCY.rl, BigDays.sfdfT[i, ], fun = mean)))
  aCIN <- c(aCIN, as.numeric(raster::extract(CIN.rl, BigDays.sfdfT[i, ], fun = mean)))
  aUSTM <- c(aUSTM, as.numeric(raster::extract(USTM.rl, BigDays.sfdfT[i, ], fun = mean)))
  aVSTM <- c(aVSTM, as.numeric(raster::extract(VSTM.rl, BigDays.sfdfT[i, ], fun = mean)))
  aBS <- c(aBS, as.numeric(raster::extract(BS.rl, BigDays.sfdfT[i, ], fun = mean)))
  aRATIO <- c(aRATIO, as.numeric(raster::extract(RATIO.rl, BigDays.sfdfT[i, ], fun = mean)))
  mCAPE <- c(mCAPE, as.numeric(raster::extract(CAPE.rl, BigDays.sfdfT[i, ], fun = max)))
  mHLCY <- c(mHLCY, as.numeric(raster::extract(HLCY.rl, BigDays.sfdfT[i, ], fun = max)))
  mCIN <- c(mCIN, as.numeric(raster::extract(CIN.rl, BigDays.sfdfT[i, ], fun = min)))
  mUSTM <- c(mUSTM, as.numeric(raster::extract(USTM.rl, BigDays.sfdfT[i, ], fun = max)))
  mVSTM <- c(mVSTM, as.numeric(raster::extract(VSTM.rl, BigDays.sfdfT[i, ], fun = max)))
  mBS <- c(mBS, as.numeric(raster::extract(BS.rl, BigDays.sfdfT[i, ], fun = max)))
}
```

Add environmental data values to the group day means data frame.
```{r, eval = FALSE}
BigDays.sfdfT$aCAPE <- aCAPE
BigDays.sfdfT$aHLCY <- aHLCY
BigDays.sfdfT$aCIN <- aCIN
BigDays.sfdfT$mCAPE <- mCAPE
BigDays.sfdfT$mHLCY <- mHLCY
BigDays.sfdfT$mCIN <- mCIN
BigDays.sfdfT$mUSTM <- mUSTM
BigDays.sfdfT$mVSTM <- mVSTM
BigDays.sfdfT$aUSTM <- aUSTM
BigDays.sfdfT$aVSTM <- aVSTM
BigDays.sfdfT$aBS <- aBS
BigDays.sfdfT$aRATIO <- aRATIO
BigDays.sfdfT$mBS <- mBS
```

Scale the variables to make them easier to read and input for models. 
```{r, eval = FALSE}
BigDays.sfdfT$aCAPE2 <- BigDays.sfdfT$aCAPE/1000
BigDays.sfdfT$aHLCY2 <- BigDays.sfdfT$aHLCY/100
BigDays.sfdfT$aCIN2 <- BigDays.sfdfT$aCIN/100
BigDays.sfdfT$aBS2 <- BigDays.sfdfT$aBS/10
BigDays.sfdfT$aUSTM2 <- BigDays.sfdfT$aUSTM/10
BigDays.sfdfT$aVSTM2 <- BigDays.sfdfT$aVSTM/10

BigDays.sfdfT$mCAPE2 <- BigDays.sfdfT$mCAPE/1000
BigDays.sfdfT$mHLCY2 <- BigDays.sfdfT$mHLCY/100
BigDays.sfdfT$mCIN2 <- BigDays.sfdfT$mCIN/100
BigDays.sfdfT$mBS2 <- BigDays.sfdfT$mBS/10
BigDays.sfdfT$mUSTM2 <- BigDays.sfdfT$mUSTM/10
BigDays.sfdfT$mVSTM2 <- BigDays.sfdfT$mVSTM/10
```

Save the data. 
```{r}
#save(BigDays.sfdfT, file = "BigDays.RData")
load("BigDays.RData")
dim(BigDays.sfdfT)
```

############
## MODELS ##
############

```{r}
library(lme4)
m1 <- lmer(log(GroupTotalED) ~ I(Year - 2004) + (1|Month),
             weights = nT, 
             data = BigDays.sfdfT)
summary(m1)

m2 <- lmer(log(GroupTotalED) ~ I(Year - 2004) + (1|Month) + mCAPE2,
             weights = nT, 
             data = BigDays.sfdfT)
summary(m2) # No to average CAPE

m3 <- lmer(log(GroupTotalED) ~ I(Year - 2004) + (1|Month) + mBS2,
             weights = nT, 
             data = BigDays.sfdfT)
summary(m3) #No to average 

m4 <- lmer(log(GroupTotalED) ~ I(Year - 2004) + (1|Month) + mHLCY2,
             weights = nT, 
             data = BigDays.sfdfT)
summary(m4) # No to average

m5 <- lmer(log(GroupTotalED) ~ I(Year - 2004) + (1|Month) + aHLCY2,
             weights = nT, 
             data = BigDays.sfdfT)
summary(m5) # No to average

m6 <- lmer(log(GroupTotalED) ~ I(Year - 2004) + (1|Month) + mCIN2,
             weights = nT, 
             data = BigDays.sfdfT)
summary(m6) # No to average

m6 <- lmer(log(GroupTotalED) ~ I(Year - 2004) + (1|Month) + mCAPE2 * mHLCY2,
             weights = nT, 
             data = BigDays.sfdfT)
summary(m6)

m7 <- lmer(log(GroupTotalED) ~ I(Year - 2004) + (1|Month) + mCAPE2 * mCIN2,
             weights = nT, 
             data = BigDays.sfdfT)
summary(m7) # No interaction with CAPE and CIN needed
```

```{r}
model1 <- lmer(log(GroupTotalED) ~ I(Year - 2004) + (1|Month) + 
                  mBS2 + mCAPE2*mHLCY2,
             weights = nT, 
             data = BigDays.sfdfT)
summary(model1)

model2 <- lmer(log(GroupTotalED) ~ I(Year - 2004) + (1|Month) + 
                  mBS2 + mCAPE2 + mHLCY2,
             weights = nT, 
             data = BigDays.sfdfT)
summary(model2)

AIC(model1, model2)
confint(model1, method = "Wald")
```

Plots for the interaction term. **NEED to FIX**
```{r}
library(interplot)
out.df <- interplot(m = model1, var1 = "mCAPE2", var2 = "mHLCY2", plot = FALSE)

out.df <- out.df %>%
  mutate(mHLCY = mHLCY2 * 100,
         coef2 = (exp(coef) - 1) * 100,
         lb2 = (exp(lb) - 1) * 100,
         ub2 = (exp(ub) - 1) * 100)

p1 <- ggplot(out.df, aes(x = mHLCY, y = coef2)) +
  geom_line() +
  geom_ribbon(aes(x = mHLCY, ymin = lb2, ymax = ub2), fill = "gray70", alpha = .3) +
  scale_x_continuous(limits = c(-500, 0)) +
  xlab(expression(paste("CIN [J/kg]"))) + 
  ylab("CAPE's Effect on Accumulated Tornado Energy\n[% Increase/1000 J/kg]") +
  theme_minimal()

p2 <- ggplot(BigDays.sfdfT, aes(x = mHLCY)) +
  geom_histogram(fill = "gray70", color = "white") +
  scale_x_continuous(limits = c(-500, 0)) +
  theme_void()

library(patchwork)
p2 + p1 + plot_layout(ncol = 1, heights = c(1, 6))
```

Locate specific values on the plot. **NEED to FIX**
```{r}
# CIN at 250 J/kg
(exp(approx(x = out.df$mCIN2, y = out.df$coef, xout = -2.5)$y) - 1) * 100
(exp(approx(x = out.df$mCIN2, y = out.df$ub, xout = -2.5)$y) - 1) * 100
(exp(approx(x = out.df$mCIN2, y = out.df$lb, xout = -2.5)$y) - 1) * 100

# CIN at 50 J/kg
(exp(approx(x = out.df$mCIN2, y = out.df$coef, xout = -.5)$y) - 1) * 100
(exp(approx(x = out.df$mCIN2, y = out.df$ub, xout = -.5)$y) - 1) * 100
(exp(approx(x = out.df$mCIN2, y = out.df$lb, xout = -.5)$y) - 1) * 100
```

Table of coefficients.
```{r}
library(xtable)
x <- summary(model1)
xtable(x$coefficients, digits = 3)
```

Observed versus predicted.
```{r}
BigDays.sfdfT$preATE <- exp(predict(model1))
cor(BigDays.sfdfT$preATE, BigDays.sfdfT$GroupTotalED)
```

Observed versus prediction plot for ATE.
```{r}
library(ggrepel)
library(viridis)

# .scresid : standardized conditional residuals
model1.df <- fortify(model1)
model1.df2 <- model1.df %>%
  filter(abs(.scresid) >= 1.7)

ggplot(model1.df, aes(x = GroupTotalED/10^9, y = preATE/10^9, color = log10(casualties + 1))) +
        # geom_smooth(method = lm, color = "black", size = .4, se = FALSE) +
#         scale_color_viridis_c(guide = FALSE, direction = -1) +
         scale_color_continuous(guide = FALSE) +
         geom_point(size = 2) + 
         geom_abline(slope = 1, size = 1) +
         #geom_label_repel(aes(x = GroupDayTotalED/10^9, y = preATE/10^9, label = as.character(cDate)), color = "black", data = model1.df2, size = 6) +
         scale_x_log10(limits = c(1, 300000), breaks = c(1, 10, 100, 1000, 10000, 100000), labels = c(1, 10, "100", "1000", "10,000", "100,000")) +
         scale_y_log10(limits = c(1, 300000), breaks = c(1, 10, 100, 1000, 10000, 100000), labels = c(1, 10, "100", "1000", "10,000", "100,000")) +
  ylab("Predicted ATE [GW]") + xlab("ATE [GW]") +
  theme_minimal() + theme(text = element_text(size=15))
```

Try to predict ATE using our casualties. Pop density on the right side. 

########################
## Population Density ##
########################

You need to establish a population density for each big day. Round the year to the nearest 10. Example: 1995 should round to 2000 census and 1993 should round to 1990 census. 

Population data are obtained from the Gridded Population of the World, version four (GPW, v4) from the Socioeconomic Data and Applications Center at Columbia University, USA. The database contain decennial census density estimates for 1990, 2000, and 2010 represented as people per square kilometer. Densities are based on residential population. The native cell resolution is .0083$^{\circ}$ latitude/longitude, which at 36$^{\circ}$ N latitude means a cell having the dimension of .9 km in the north-south direction and .7 km in the east-west direction. 

Load the population raster(s) and crop to defined extent. Use the `extract` function to obtain the population density. The `projectRaster` and `extract` functions take about 10 minutes per raster. Group years by mid-year of the census year. Assign the population density using the first decennial estimate then successively replace with later estimates by grouped years.
```{r}
stime = proc.time()
PopD2000 = raster("gpw-v4-population-density_2000.tif")
PopD2010 = raster("gpw-v4-population-density_2010.tif")
ext = raster::extent(c(-125, -67, 24, 50))
PopD2000 = crop(PopD2000, ext)
PopD2010 = crop(PopD2010, ext)

PopD2000p = projectRaster(PopD2000, crs = proj4string(TornP))
PopD2010p = projectRaster(PopD2010, crs = proj4string(TornP))
TornP$popD2000 = raster::extract(PopD2000p, TornP, fun = mean, na.rm = TRUE,
                                       weights = TRUE, normalizeWeights = FALSE)[, 1]
TornP$popD2010 = raster::extract(PopD2010p, TornP, fun = mean, na.rm = TRUE,
                                       weights = TRUE, normalizeWeights = FALSE)[, 1]
TornP$popD = TornP$popD2000
yearGroup = TornP$yr > 2005
TornP$popD[yearGroup] = TornP$popD2010[yearGroup]

TornP$pop = TornP$popD * TornP$AreaPath/10^6

Torn.df = as.data.frame(TornP)
proc.time() - stime
```

Begin with The Gridded Population of the World Volume 3 data (2000).
```{r}
#TornLs = subset(TornL, date == "2011-04-27")
TornP = gBuffer(TornL, byid = TRUE, width = TornL$Width/2, capStyle = "FLAT")
Pop = raster("/Users/tylerfricker/Dropbox/Tyler/usadens/usads00g/w001001.adf")
Popp = projectRaster(Pop, crs = proj4string(TornP))
TornP$popD = as.vector(raster::extract(Popp, TornP, fun = mean, na.rm = TRUE,
                                       weights = FALSE, normalizeWeights = FALSE))
TornP$pop = TornP$popD * TornP$AreaPath/10^6
Torn.df = as.data.frame(TornP)
```


Note: The warning results from tornadoes with paths over areas with no people. It can be safely ignored.

The `popD` attribute for each tornado corresponds to the population density from the nearest decennial estimate so that a tornado that occurred in 1996 uses the 2000 decennial estimate while a tornado that occurred in 2012 uses the 2010 estimate.

For the set of 2208 tornadoes with at least one casualty the median population density per tornado is 31.3 people per square kilometer with an inter-quartile range between 9.6 and 136 people per square kilometer.

1 mi^2 = 2.589988 km^2
http://mcdc.missouri.edu/TenThings/urbanrural.shtml

```{r}
Torn.df %>%
  summarize(nT = n(),
            avgPopD = mean(popD),
            medianPopD = median(popD),
            q25 = quantile(popD, probs = .25),
            q75 = quantile(popD, probs = .75),
            maxPopD = max(popD),
            minPopD = min(popD)) %>%
  t()

Torn.df %>%
  dplyr::select(date, st, mag, inj, fat, cas, popD) %>%
  arrange(desc(popD)) %>%
  head(., n = 10) %>%
  xtable(., digits = 0)

sum(Torn.df$popD == 0)

df = Torn.df %>%
  filter(yr >= 1990 & yr <= 2001) %>%
  summarize(totalPop = sum(pop))
```

Annual average number of people exposed to tornadoes that caused at least one casualty.
```{r}
df = Torn.df %>%
  filter(cas > 0) %>%
  group_by(Year) %>%
  summarize(nT = n(),
            mPop = mean(pop),
            sdPop = sd(pop),
            sePop = sdPop/sqrt(nT),
            mPopm = mPop - sePop,
            mPopp = mPop + sePop)
A = ggplot(df, aes(x = Year, y = mPop)) +
  geom_point() +
  geom_errorbar(aes(ymin = mPopm, ymax = mPopp), width = .2) +
  scale_y_continuous(limits = c(0, 2500)) +
  scale_x_continuous(breaks = seq(1995, 2016, 4)) +
  xlab("Year") + ylab("Number of People Exposed") +
  theme_minimal()
```

##################
## SPC Outlooks ##
##################

The SPC archive data extends from January 23, 2003 to present. (https://www.spc.noaa.gov/archive/)

Data File Example: 
https://www.spc.noaa.gov/cgi-bin-spc/getacrange.pl?date0=20110426&date1=20110426&csrf=0e9284c1e6fff8bc469a08cd5379c18197cc4fec/day1otlk_20110426_1630-shp.zip 
